---
share: true
---
# [\#192 Issue](https://github.com/extratone/bilge/issues/192) `open`: GitHub CoPilot
**Labels**: `documentation`, `dev`


#### <img src="https://avatars.githubusercontent.com/u/43663476?u=5047287ff0b8c3ce7f7e5858d204c9b3e57d8e44&v=4" width="50">[extratone](https://github.com/extratone) opened issue at [2021-07-14 19:33](https://github.com/extratone/bilge/issues/192):

![image](https://user-images.githubusercontent.com/43663476/125681988-2d844d2b-6a40-4113-8da6-ae9bd7d81af5.png)


#### <img src="https://avatars.githubusercontent.com/u/43663476?u=5047287ff0b8c3ce7f7e5858d204c9b3e57d8e44&v=4" width="50">[extratone](https://github.com/extratone) commented at [2021-07-14 19:33](https://github.com/extratone/bilge/issues/192#issuecomment-880154532):

# "GitHub‚Äôs Commercial AI Tool Was Built From Open Source Code"

*14-07-2021 14:32* 

> Copilot is pitched as a helpful aid to developers. But some programmers object to the blind copying of blocks of code used to train the algorithm.
Earlier this month, Armin Ronacher, a prominent [open-source](https://www.wired.com/tag/open-source/) developer, was experimenting with a new code-generating tool from [GitHub](https://www.wired.com/tag/github/) called Copilot when it began to produce a curiously familiar stretch of code. The lines, drawn from the source code of the 1999 video game *Quake III*, are infamous among programmers‚Äîa combo of little tricks that add up to some pretty basic math, imprecisely. The original *Quake* coders knew they were hacking. ‚ÄúWhat the fuck,‚Äù one commented in the code beside an especially egregious shortcut.

So it was strange for Ronacher to see such code generated by Copilot, an [artificial intelligence](https://www.wired.com/tag/artificial-intelligence/) tool that is marketed to generate code that is both novel and efficient. The AI was plagiarizing‚Äîcopying the hack (including the profane comment) verbatim. Worse yet, the code it had chosen to copy was under copyright protection. Ronacher [posted a screenshot to Twitter](https://twitter.com/mitsuhiko/status/1410886329924194309), where it was entered as evidence in a roiling trial-by-social-media over whether Copilot is exploiting programmers‚Äô labor.

Copilot, which GitHub calls ‚Äú[your AI pair programmer](https://copilot.github.com/),‚Äù is the result of a collaboration with [OpenAI](https://www.wired.com/tag/openai/), the formerly nonprofit research lab known for powerful language-generating AI models such as GPT-3. At its heart is a [neural network](https://www.wired.com/tag/neural-networks/) that is trained using massive volumes of data. Instead of text, though, Copilot‚Äôs source material is code: millions of lines uploaded by the 65 million users of GitHub, the world‚Äôs largest platform for developers to collaborate and share their work. The aim is for Copilot to learn enough about the patterns in that code that it can do some hacking itself. It can take the incomplete code of a human partner and finish the job. For the most part, it appears successful at doing so. GitHub, which was purchased by [Microsoft](https://www.wired.com/tag/microsoft/) in 2018, plans to sell access to the tool to developers.

To many programmers, Copilot is exciting because coding is hard. While AI can now generate photo-realistic faces and write plausible essays in response to prompts, code has been largely untouched by those advances. An AI-written text that reads strangely might be embraced as ‚Äúcreative,‚Äù but code offers less margin for error. A bug is a bug, and it means the code could have a security hole or a memory leak, or more likely that it just won‚Äôt work. But writing correct code also demands a balance. The system can‚Äôt simply regurgitate verbatim code from the data used to train it, especially if that code is protected by copyright. That‚Äôs not AI code generation; that‚Äôs plagiarism.

GitHub says Copilot‚Äôs slip-ups are only occasional, but critics say the blind copying of code is less of an issue than what it reveals about AI systems generally: Even if code is not copied directly, should it have been used to train the model in the first place? GitHub has been unclear about precisely which code was involved in training Copilot, but it has clarified its stance on the principles as the debate over the tool has unfolded: All publicly available code is fair game regardless of its copyright.

That hasn‚Äôt sat well with some GitHub users who say the tool both depends on their code and ignores their wishes for how it will be used. The company has taken both free-to-use and copyrighted code and ‚Äúput it all in a blender in order to sell the slurry to commercial and proprietary interests,‚Äù says Evelyn Woods, a Colorado-based programmer and game designer [whose tweets](https://twitter.com/eevee/status/1410037309848752128) on the topic went viral. ‚ÄúIt feels like it‚Äôs laughing in the face of open source.‚Äù

AI tools bring industrial scale and automation to an old tension at the heart of open source programming: Coders want to share their work freely under permissive licenses, but they worry that the chief beneficiaries will be large businesses that have the scale to profit from it. A corporation takes a young startup‚Äôs free-to-use code to corner a market or uses an open source library without helping with the maintenance. Code-generating AI systems that rely on large data sets mean everyone‚Äôs code is potentially subject to reuse for commercial applications.

‚ÄúI‚Äôm generally happy to see expansions of free use, but I‚Äôm a little bitter when they end up benefiting massive corporations who are extracting value from smaller authors‚Äô work en masse,‚Äù Woods says.

One thing that‚Äôs clear about neural networks is that they can memorize their training data and reproduce copies. That risk is there regardless of whether that data involves personal information or medical secrets or copyrighted code, explains Colin Raffel, a professor of computer science at the University of North Carolina who ¬†coauthored a forthcoming paper (currently available as a [non-peer-reviewed preprint](https://arxiv.org/abs/2012.07805)) examining similar copying in OpenAI‚Äôs GPT-2. Getting the model, which is trained on a large corpus of text, to spit out training data was rather trivial, they found. But it can be difficult to predict what a model will memorize and copy. ‚ÄúYou only really find out when you throw it out into the world and people use and abuse it,‚Äù Raffel says. Given that, he was surprised to see that GitHub and OpenAI had chosen to train their model with code that came with copyright restrictions.

According to [GitHub‚Äôs internal tests](https://docs.github.com/en/github/copilot/research-recitation), direct copying occurs in roughly 0.1 percent of Copilot‚Äôs outputs‚Äîa surmountable error, according to the company, and not an inherent flaw in the AI model. That‚Äôs enough to cause a nit in the legal department of any for-profit entity (‚Äúnon-zero risk‚Äù is just ‚Äúrisk‚Äù to a lawyer), but Raffel notes this is perhaps not all that different from employees copy-pasting restricted code. Humans break the rules regardless of automation. Ronacher, the open source developer, adds that most of Copilot‚Äôs copying appears to be relatively harmless‚Äîcases where simple solutions to problems come up again and again, or oddities like the infamous *Quake* code, which has been (improperly) copied by people into many different codebases. ‚ÄúYou can make Copilot trigger hilarious things,‚Äù he says. ‚ÄúIf it‚Äôs used as intended I think it will be less of an issue.‚Äù

GitHub has also indicated it has a possible solution in the works: a way to flag those verbatim outputs when they occur so that programmers and their lawyers know not to reuse them commercially. But building such a system is not as simple as it sounds, Raffel notes, and it gets at the larger problem: What if the output is not verbatim, but a near copy of the training data? What if only the variables have been changed, or a single line has been expressed in a different way? In other words, how much change is required for the system to no longer be a copycat? With code-generating software in its infancy, the legal and ethical boundaries aren‚Äôt yet clear.

### See What‚Äôs Next in Tech With the Fast Forward Newsletter

From artificial intelligence and self-driving cars to transformed cities and new startups, sign up for the latest news.

Many legal scholars believe AI developers have fairly wide latitude when selecting training data, explains Andy Sellars, director of Boston University‚Äôs Technology Law Clinic. ‚ÄúFair use‚Äù of copyrighted material largely boils down to whether it is ‚Äútransformed‚Äù when it is reused. There are many ways of transforming a work, like using it for parody or criticism or summarizing it‚Äîor, as courts have repeatedly found, using it as the fuel for algorithms. In one prominent case, a federal court [rejected a lawsuit](https://www.lexisnexis.com/community/casebrief/p/casebrief-authors-guild-v-google-inc) brought by a publishing group against Google Books, holding that its process of scanning books and using snippets of text to let users search through them was an example of fair use. But how that translates to AI training data isn‚Äôt firmly settled, Sellars adds.

It‚Äôs a little odd to put code under the same regime as books and artwork, he notes. ‚ÄúWe treat source code as a literary work even though it bears little resemblance to literature,‚Äù he says. We may think of code as comparatively utilitarian; the task it achieves is more important than how it is written. But in copyright law, the key is how an idea is expressed. ‚ÄúIf Copilot spits out an output that does the same thing as one of its training inputs does‚Äîsimilar parameters, similar result‚Äîbut it spits out different code, that‚Äôs probably not going to implicate copyright law,‚Äù he says.

The ethics of the situation are another matter. ‚ÄúThere‚Äôs no guarantee that GitHub is keeping independent coders‚Äô interests to heart,‚Äù Sellars says. Copilot depends on the work of its users, including those who have explicitly tried to prevent their work from being reused for profit, and it may also reduce demand for those same coders by automating more programming, he notes. ‚ÄúWe should never forget that there is no cognition happening in the model,‚Äù he says. It‚Äôs statistical pattern matching. The insights and creativity mined from the data are all human. Some [scholars have said](https://www.psagroup.org/blogposts/101) that Copilot underlines the need for new mechanisms to ensure that those who produce the data for AI are fairly compensated.

GitHub declined to answer questions about Copilot and directed me to an FAQ about the system. In a [series of posts](https://news.ycombinator.com/item?id=27677177) on Hacker News, GitHub CEO Nat Friedman responded to the developer outrage by projecting confidence about the fair use designation of training data, pointing to an [OpenAI position paper](https://www.uspto.gov/sites/default/files/documents/OpenAI_RFC-84-FR-58141.pdf) on the topic. GitHub was ‚Äúeager to participate‚Äù in coming debates over AI and intellectual property, he wrote.

Ronacher says that he expects advocates of free software to defend Copilot‚Äîand indeed, some [already have](https://juliareda.eu/2021/07/github-copilot-is-not-infringing-your-copyright/)‚Äîout of concern that drawing limits on fair use could jeopardize the free sharing of software more broadly. But it‚Äôs unclear if the tool will spark meaningful legal challenges that clarify the fair use issues anytime soon. The kind of tasks people are tackling with Copilot are mostly boilerplate, Ronacher points out‚Äîunlikely to run afoul of anyone. But for him, that‚Äôs part of why the tool is exciting, because it means automating away annoying tasks. He already uses permissive licenses whenever he can in the hopes that other developers will pluck out whatever is useful, and Copilot could help automate that sharing process. ‚ÄúAn engineer shouldn‚Äôt waste two hours of their life implementing a function I‚Äôve already done,‚Äù he says.

But Ronacher can see the challenges. ‚ÄúIf you‚Äôve spent your life doing something, you expect something for it,‚Äù he says. At Sentry, a debugging software startup where he is director of engineering, the team recently tightened some of its most permissive licenses‚Äîwith great reluctance, he says‚Äîfor fear that ‚Äúa large company like Amazon could just run away with our stuff.‚Äù As AI applications advance, those companies are poised to run faster.

***

More Great WIRED Stories

-   üì© The latest on tech, science, and more: [Get our newsletters](https://www.wired.com/newsletter?sourceCode=BottomStories)!
-   The ride-hailing legend who tried to [outfox the gig economy](https://www.wired.com/story/gig-economy-uber-lyft-doordash-jeffrey-fang/?itm_campaign=BottomRelatedStories&itm_content=footer-recirc)
-   Help! How do I accept that [I'm burned out?](https://www.wired.com/story/ooo-i-might-be-burned-out-and-dont-know-what-to-do/?itm_campaign=BottomRelatedStories&itm_content=footer-recirc)
-   What you need to [edit studio-grade home videos](https://www.wired.com/story/gear-and-tips-to-make-professional-videos/?itm_campaign=BottomRelatedStories&itm_content=footer-recirc)
-   [Florida's condo collapse](https://www.wired.com/story/floridas-condo-collapse-foreshadows-the-concrete-crack-up/?itm_campaign=BottomRelatedStories&itm_content=footer-recirc) signals the concrete crack-up
-   How [underground fiber optics](https://www.wired.com/story/how-underground-fiber-optics-spy-on-humans-moving-above/?itm_campaign=BottomRelatedStories&itm_content=footer-recirc) spy on humans above
-   üëÅÔ∏è Explore AI like never before with [our new database](https://www.wired.com/category/artificial-intelligence/?itm_campaign=BottomRelatedStories&itm_content=footer-recirc)
-   üéÆ WIRED Games: Get the latest [tips, reviews, and more](https://www.wired.com/tag/video-games/?itm_campaign=BottomRelatedStories&itm_content=footer-recirc)
-   üíª Upgrade your work game with our Gear team‚Äôs [favorite laptops](https://www.wired.com/gallery/best-laptops/?itm_campaign=BottomRelatedStories&itm_content=footer-recirc), [keyboards](https://www.wired.com/story/top-3-mechanical-keyboards/?itm_campaign=BottomRelatedStories&itm_content=footer-recirc), [typing alternatives](https://www.wired.com/story/best-keyboard-alternatives/?itm_campaign=BottomRelatedStories&itm_content=footer-recirc), and [noise-canceling headphones](https://www.wired.com/gallery/best-noise-canceling-headphones/?itm_campaign=BottomRelatedStories&itm_content=footer-recirc)
***

==**11179**== Words

- **[GitHub‚Äôs Commercial AI Tool Was Built From Open Source Code | WIRED](https://www.wired.com/story/github-commercial-ai-tool-built-open-source-code/)**


-------------------------------------------------------------------------------



[Export of Github issue for [extratone/bilge](https://github.com/extratone/bilge). Generated on 2022.05.04 at 17:49:42.]
